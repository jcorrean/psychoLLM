# <span style="color: #7c0cc7;">Applying Information Theory</span>

First, let's retrieve the results we got previously. Here, I focus on the text-features matrix representing the linguistic corpus of the eight items generated by humans "ExistingItems." In this matrix, every word is deployed as a column, and every item is deployed as a numbered text (row).

```{r, warning=FALSE, message=FALSE}
load("Data/DocumentFeaturesMatrices.RData")
My_matrix <- as.matrix(ExistingItems)
My_matrix
```

Now, we compute the mutual information from this matrix

```{r, warning=FALSE, message=FALSE}
library(entropy)
mi.empirical(My_matrix)
```

The result we achieve, 0.6425168, represents the average mutual information across all pairs of features (words) in our matrix. Here's what this number means in practical terms:

Low values close to zero indicate that there is little to no relationship between these features. Knowing the value of one feature doesn't give you much information about the value of another. In contrast, high values, like the one we have (0.6425168), suggest that there is a strong relationship between many pairs of features. This could mean that words or phrases tend to co-occur frequently or that the frequency of one word or phrase is highly predictive of the frequency of another. This can be seen as an equivalent empirical guidance to gauge items similarity. 

Now, let's focus on the matrix for the items generated by Claude (I called it as "LLMGeneratedItems"). This time, the matrix is larger than the previous one, because it has 20 items (rows) and 100 features (columns).

```{r}
dim(LLMGeneratedItems)
Matrix2 <- as.matrix(LLMGeneratedItems)
mi.empirical(Matrix2)
```

This result suggests that the second matrix has a stronger overall relationship between its features compared to the first matrix.

Implications

Stronger Relationships: A higher average mutual information implies that there are stronger relationships between the features in the second matrix. This could mean that the features (e.g., words or phrases) in this matrix are more closely associated with each other, either because they co-occur more frequently or because the frequency of one feature is more predictive of the frequency of others.

Now, let's apply the same analysis to the entire set of items (integrating those generated by humans and those generated by Claude).

```{r}
Matrix3 <- as.matrix(CLAUDEITEMS)
mi.empirical(Matrix3)
```

