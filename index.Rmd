---
title: "Informational Analysis of LLM-generated Psychometric Items"
author: 
  - name: Juan C. Correa
email: jcc@criticalcentrality.com
affiliation: Critical Centrality Institute
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
bookdown::gitbook:
  css: assets/custom.css
toc_depth: 1
documentclass: book
bibliography: [book.bib]
biblio-style: apacite
link-citations: TRUE
---
# <span style="color: #7c0cc7;">Motivation</span> {.unnumbered}

Everyone has heard about "Large Language Models" (LLMs). These models rely on a generative artificial intelligence technology known as "_Generative Pre-Trained Transformer_." Such technology is the foundational basis of "ChatGPT" [@Mitchell2023]. 

For applied psychologists in general and psychometricians in particular, LLMs can be useful for psychometric items generation. Unfortunately, the empirical evidence for such purposes is scarce, until now. Here, I report a short piece of empirical evidence that shows how psychologists can apply "<span style="color: #7c0cc7;">**information theory**</span>" to grasp a quantitative metric of the quality of these items as compared to those generated by humans.